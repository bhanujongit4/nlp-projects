{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d kazanova/sentiment140\n",
        "\n",
        "# Unzip the downloaded file\n",
        "!unzip sentiment140.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMQDrU26mrMZ",
        "outputId": "c6df8473-f988-42db-90bf-aced6bb850cd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/kazanova/sentiment140\n",
            "License(s): other\n",
            "Downloading sentiment140.zip to /content\n",
            " 98% 79.0M/80.9M [00:05<00:00, 21.8MB/s]\n",
            "100% 80.9M/80.9M [00:05<00:00, 16.5MB/s]\n",
            "Archive:  sentiment140.zip\n",
            "  inflating: training.1600000.processed.noemoticon.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load and preprocess the data\n",
        "df = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding='latin-1', header=None)\n",
        "df.columns = ['target', 'id', 'date', 'flag', 'user', 'text']\n",
        "df = df[['target', 'text']]\n",
        "df['target'] = df['target'].map({0: 0, 4: 1})\n",
        "\n",
        "# Tokenization and vocabulary building\n",
        "def build_vocab(texts, max_words=10000):\n",
        "    word_freq = {}\n",
        "    for text in texts:\n",
        "        for word in text.lower().split():\n",
        "            word_freq[word] = word_freq.get(word, 0) + 1\n",
        "\n",
        "    vocab = ['<PAD>', '<UNK>'] + sorted(word_freq, key=word_freq.get, reverse=True)[:max_words-2]\n",
        "    word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "    return word_to_idx\n",
        "\n",
        "word_to_idx = build_vocab(df['text'])\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "def tokenize(text, word_to_idx, max_len=100):\n",
        "    tokens = [word_to_idx.get(word, word_to_idx['<UNK>']) for word in text.lower().split()[:max_len]]\n",
        "    if len(tokens) < max_len:\n",
        "        tokens += [word_to_idx['<PAD>']] * (max_len - len(tokens))\n",
        "    return tokens\n",
        "\n",
        "X = np.array([tokenize(text, word_to_idx) for text in df['text']])\n",
        "y = df['target'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# PyTorch Dataset\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = torch.LongTensor(texts)\n",
        "        self.labels = torch.LongTensor(labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx], self.labels[idx]\n",
        "\n",
        "# Transformer Block\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ff_dim, embed_dim)\n",
        "        )\n",
        "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
        "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_output, _ = self.attention(x, x, x)\n",
        "        x = self.layernorm1(x + self.dropout(attn_output))\n",
        "        ff_output = self.ff(x)\n",
        "        x = self.layernorm2(x + self.dropout(ff_output))\n",
        "        return x\n",
        "\n",
        "# Sentiment Analysis Model\n",
        "class SentimentModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, ff_dim, num_layers, max_len, num_classes):\n",
        "        super(SentimentModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_encoding = nn.Parameter(torch.randn(1, max_len, embed_dim))\n",
        "        self.transformer_blocks = nn.ModuleList(\n",
        "            [TransformerBlock(embed_dim, num_heads, ff_dim) for _ in range(num_layers)]\n",
        "        )\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x) + self.pos_encoding\n",
        "        x = x.permute(1, 0, 2)  # (seq_len, batch, embedding_dim)\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x)\n",
        "        x = x.permute(1, 2, 0)  # (batch, embedding_dim, seq_len)\n",
        "        x = self.global_avg_pool(x).squeeze(-1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Hyperparameters\n",
        "vocab_size = len(word_to_idx)\n",
        "embed_dim = 64\n",
        "num_heads = 8\n",
        "ff_dim = 512\n",
        "num_layers = 4\n",
        "max_len = 100\n",
        "num_classes = 2\n",
        "batch_size = 64\n",
        "num_epochs = 2\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = SentimentDataset(X_train, y_train)\n",
        "test_dataset = SentimentDataset(X_test, y_test)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = SentimentModel(vocab_size, embed_dim, num_heads, ff_dim, num_layers, max_len, num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_acc = 0\n",
        "    for batch_x, batch_y in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
        "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_x)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        train_acc += (predicted == batch_y).sum().item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    train_acc /= len(train_loader.dataset)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_acc = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in test_loader:\n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "            outputs = model(batch_x)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            val_acc += (predicted == batch_y).sum().item()\n",
        "\n",
        "    val_loss /= len(test_loader)\n",
        "    val_acc /= len(test_loader.dataset)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}:')\n",
        "    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
        "    print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), 'sentiment_model.pth')\n",
        "\n",
        "# Function to predict sentiment with confidence\n",
        "def predict_sentiment(text):\n",
        "    model.eval()\n",
        "    tokens = torch.LongTensor([tokenize(text, word_to_idx)]).to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model(tokens)\n",
        "        probabilities = torch.nn.functional.softmax(output, dim=1)\n",
        "        confidence, prediction = torch.max(probabilities, 1)\n",
        "    return prediction.item(), confidence.item()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82NpFBOMn25p",
        "outputId": "e8bde990-9af9-45a7-b397-abe914720e53"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2: 100%|██████████| 20000/20000 [06:32<00:00, 50.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2:\n",
            "Train Loss: 0.4686, Train Acc: 0.7732\n",
            "Val Loss: 0.4355, Val Acc: 0.7957\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/2: 100%|██████████| 20000/20000 [06:26<00:00, 51.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/2:\n",
            "Train Loss: 0.4282, Train Acc: 0.8002\n",
            "Val Loss: 0.4290, Val Acc: 0.7999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "texts = [\n",
        "    \"Oh no! aliens are here\",\n",
        "    \"Oh yes! santa is here\",\n",
        "    \"Hitler isnt dead\",\n",
        "    \"The food was dry. \"\n",
        "]\n",
        "\n",
        "for text in texts:\n",
        "    sentiment, confidence = predict_sentiment(text)\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Sentiment: {'Positive' if sentiment == 1 else 'Negative'}\")\n",
        "    print(f\"Confidence: {confidence:.2f}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0piHZrFn7I-",
        "outputId": "aea045fe-3037-4ff6-9fdb-5791a3c15a37"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Oh no! aliens are here\n",
            "Sentiment: Negative\n",
            "Confidence: 0.92\n",
            "\n",
            "Text: Oh yes! santa is here\n",
            "Sentiment: Positive\n",
            "Confidence: 0.96\n",
            "\n",
            "Text: Hitler isnt dead\n",
            "Sentiment: Negative\n",
            "Confidence: 0.89\n",
            "\n",
            "Text: The food was dry. \n",
            "Sentiment: Negative\n",
            "Confidence: 0.76\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('word_to_idx.pkl', 'wb') as f:\n",
        "    pickle.dump(word_to_idx, f)\n"
      ],
      "metadata": {
        "id": "x_BJAMhdSj4g"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}